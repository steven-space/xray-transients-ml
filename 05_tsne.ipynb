{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 00:38:01.743666: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64366fa8f344f493424586a02310f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Set :', options=('All', 'Bona'), value='All')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PYTHON Imports \n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import fnmatch\n",
    "# ASTROPHY Imports\n",
    "import astropy \n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "from sherpa.astro import ui\n",
    "# CIAO Imports\n",
    "import ciao_contrib.runtool\n",
    "from ciao_contrib.runtool import *\n",
    "# CUSTOM Imports\n",
    "from data_extraction_functions import *\n",
    "from data_exploration_functions import *\n",
    "from data_representation_functions import *\n",
    "\n",
    "# Specify global path\n",
    "global_path = '/Users/steven/Library/Mobile Documents/com~apple~CloudDocs/0-CfA/4-Data/Datasets'\n",
    "global_folders = list_folders_fun(global_path)\n",
    "\n",
    "# Custom object hook to convert lists of lists to NumPy arrays\n",
    "def numpy_hook(obj):\n",
    "    if isinstance(obj, list):\n",
    "        # Check if the list contains sublists (i.e. a matrix)\n",
    "        if isinstance(obj[0], list):\n",
    "            # Convert the list of lists to a NumPy array matrix\n",
    "            return np.array(obj)\n",
    "    # Return all other objects as is\n",
    "    return obj\n",
    "\n",
    "# Select dataset\n",
    "set_widget = widgets.Dropdown(options=global_folders[:],value=global_folders[0],description='Set :',disabled=False); set_widget"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70c29eb59c942efa89646d81524fe4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='TSNE File :', options=('hist2D-All-nE16-nt24-normnone.pkl', 'hist3D-All-nE16-nt24-ndt24-â€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set ID\n",
    "set_id = set_widget.value\n",
    "# Select Input\n",
    "files = os.listdir(f'{global_path}/{set_id}/')\n",
    "input_files = [f for f in files if fnmatch.fnmatch(f, 'hist*nE16*.pkl')]\n",
    "input_widget = widgets.Dropdown(options=input_files[:],value=input_files[0],description='TSNE File :',disabled=False); input_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  82283\n",
      "Number of Property Sets:  82283\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "input_file = input_widget.value\n",
    "# Load histogram dictionary\n",
    "with open(f'{global_path}/{set_id}/{input_file}', 'rb') as f:\n",
    "    hist_dict = pickle.load(f)\n",
    "# Flatten histograms in the dictionary and get IDs\n",
    "ids = hist_dict.keys()\n",
    "histograms = hist_dict.values()\n",
    "features = np.array([np.array(h).flatten() for h in histograms])\n",
    "features[np.isnan(features)] = 0.0\n",
    "# Load properties\n",
    "df_properties_input = pd.read_csv(f'{global_path}/{set_id}/properties-input-{set_id}.csv')\n",
    "df_properties_input = df_properties_input[df_properties_input['obsreg_id'].isin(list(ids))]\n",
    "df_properties = df_properties_input.drop_duplicates('obsreg_id', keep='first').reset_index()\n",
    "\n",
    "# Print eventfiles and properties number of IDs\n",
    "print(\"Number of Features: \", len(features))\n",
    "print(\"Number of Property Sets: \", len(df_properties))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82273\n",
      "51830\n",
      "51830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select only the columns you need as labels from df_properties\n",
    "df_label = df_properties[['obsreg_id','hard_hm', 'hard_hs', 'hard_ms', 'var_prob_b', 'var_prob_h', 'var_prob_m', 'var_prob_s']]\n",
    "\n",
    "mask_nonan = df_label.notna().all(axis=1)\n",
    "index_nonan = list(df_label.notna().all(axis=1).index[df_label.notna().all(axis=1)])\n",
    "print(max(index_nonan))\n",
    "df_label = df_label[mask_nonan]\n",
    "ID = df_label['obsreg_id'].values\n",
    "X = features[(index_nonan),:]\n",
    "# Scale the labels to the same range as the data\n",
    "scaler = MinMaxScaler()\n",
    "df_y = df_label[['hard_hm', 'hard_hs', 'hard_ms', 'var_prob_b', 'var_prob_h', 'var_prob_m', 'var_prob_s']]\n",
    "Y = df_y.values\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(100)\n",
    "X = pca_model.fit_transform(X) \n",
    "\n",
    "# identify the elbow point\n",
    "cumulative_var = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "elbow_point = np.argmax(np.diff(cumulative_var) <= 0.005) + 1\n",
    "n_components95 = np.argmax(cumulative_var >= 0.95) + 1\n",
    "avg = (elbow_point  + n_components95)/2\n",
    "print('Elbow point:', elbow_point)\n",
    "print(\"Number of components to retain 95% variance:\", n_components95)\n",
    "print(\"Avg:\", avg)\n",
    "\n",
    "# plot the scree plot\n",
    "ax1 = plt.plot(np.arange(1, pca_model.n_components_ + 1), pca_model.explained_variance_ratio_, 'o-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(False) \n",
    "plt.axvline(x=elbow_point, color='r')\n",
    "plt.axvline(x=n_components95, color='b')\n",
    "plt.axvline(x=avg, color='g')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the cumulative proportion of variance explained\n",
    "ax2 = plt.plot(np.arange(1, pca_model.n_components_ + 1), np.cumsum(pca_model.explained_variance_ratio_), 'o-')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Proportion of Variance Explained')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(False) \n",
    "plt.axvline(x=elbow_point, color='r')\n",
    "plt.axvline(x=n_components95, color='b')\n",
    "plt.axvline(x=avg, color='g')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA DONE\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "import dcor\n",
    "# k = 50\n",
    "# samples = 5000\n",
    "# ran = random.randint(samples, 80000)-samples\n",
    "# X = X[ran:ran+samples,:]\n",
    "# Y = Y[ran:ran+samples,:]\n",
    "samples = 51830\n",
    "\n",
    "pca_model = PCA(21)\n",
    "PCA = pca_model.fit_transform(X) \n",
    "print('PCA DONE')\n",
    "\n",
    "# define the list of learning rates and perplexities to try\n",
    "# learning_rates = [50, 100, 250, 500, 750, 1000, 2500, 5000,10000]\n",
    "learning_rates = [25, 50, 75, 100, 125,150,175,200,225,250]\n",
    "# perplexities = [25, 50, 100, 250, 500, 750, 1000, 2500,5000]\n",
    "perplexities = [10,20,30,40,50,60,70,80,90,100]\n",
    "performance_matrix = np.zeros((len(learning_rates), len(perplexities)))\n",
    "performance_matrix2 = np.zeros((len(learning_rates), len(perplexities)))\n",
    "performance_matrix3 = np.zeros((len(learning_rates), len(perplexities)))\n",
    "performance_matrix4 = np.zeros((len(learning_rates), len(perplexities)))\n",
    "performance_matrix5 = np.zeros((len(learning_rates), len(perplexities)))\n",
    "performance_matrix6 = np.zeros((len(learning_rates), len(perplexities)))\n",
    "rec = []\n",
    "prec = []\n",
    "\n",
    "# Compute the inverse covariance matrix\n",
    "covariance_matrix = np.cov(Y, rowvar=False)\n",
    "inverse_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "# Compute the Mahalanobis distance matrix for X using pdist\n",
    "dY2 = pdist(Y, 'mahalanobis', VI=inverse_covariance_matrix)\n",
    "dY = squareform(dY2)\n",
    "\n",
    "total_count = len(learning_rates) * len(perplexities)\n",
    "counter = 0\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, perp in enumerate(perplexities):\n",
    "        # Compute Xtsne embedding\n",
    "        tsne = TSNE(learning_rate=lr, perplexity=perp, n_iter = 2000, early_exaggeration = 1, init='random')\n",
    "        X_tsne = tsne.fit_transform(PCA)\n",
    "        # # Nearest neighbor models\n",
    "        # nn_X = NearestNeighbors(n_neighbors=k)\n",
    "        # nn_Y = NearestNeighbors(n_neighbors=k)\n",
    "        # nn_X.fit(X)\n",
    "        # nn_Y.fit(Y)\n",
    "        # _, indices_X = nn_X.kneighbors(X)\n",
    "        # _, indices_Y = nn_Y.kneighbors(Y)\n",
    "        # # Compute recall and precision at k\n",
    "        # recall_at_k = np.mean([np.intersect1d(indices_X[i], indices_Y[i]).shape[0] / k for i in range(samples)])\n",
    "        # precision_at_k = np.mean([np.intersect1d(indices_X[i], indices_Y[i]).shape[0] / indices_X[i].shape[0] for i in range(samples)])\n",
    "        # rec.append(recall_at_k)\n",
    "        # prec.append(precision_at_k)\n",
    "        # f1 = (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
    "        # performance_matrix3[i,j] = f1\n",
    "        # compute pairwise distances\n",
    "\n",
    "\n",
    "        dX2 = pdist(X_tsne,'euclidean')\n",
    "        dX = squareform(dX2)\n",
    "        performance_metric = spearmanr(dX.flatten(), dY.flatten())[0]\n",
    "        performance_metric2 = pearsonr(dX.flatten(), dY.flatten())[0]\n",
    "        performance_metric3 = spearmanr(dX2.flatten(), dY2.flatten())[0]\n",
    "        performance_metric4 = pearsonr(dX2.flatten(), dY2.flatten())[0]\n",
    "        performance_matrix[i,j] = performance_metric \n",
    "        performance_matrix2[i,j] = performance_metric2 \n",
    "        performance_matrix3[i,j] = performance_metric3\n",
    "        performance_matrix4[i,j] = performance_metric4\n",
    "\n",
    "        dcor_coefficient1 = dcor.distance_correlation(dX, dY)\n",
    "        dcor_coefficient2 = dcor.distance_correlation(dX2, dY2)\n",
    "        performance_matrix5[i,j] = dcor_coefficient1\n",
    "        performance_matrix6[i,j] = dcor_coefficient2\n",
    "\n",
    "\n",
    "        counter = counter+1\n",
    "        clear_output(wait=True)\n",
    "        print(f'Counter: {counter} of {total_count}')\n",
    "        print(f'DONE!!!')\n",
    "\n",
    "# save the matrix to a file\n",
    "input_file = input_widget.value\n",
    "data_rep = input_file.split(\".pkl\")[0]\n",
    "with open(f'{global_path}/{set_id}/learn-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(learning_rates, f)\n",
    "with open(f'{global_path}/{set_id}/perp-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(perplexities, f)\n",
    "with open(f'{global_path}/{set_id}/perfmatrix-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix, f)\n",
    "with open(f'{global_path}/{set_id}/perfmatrix2-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix2, f)\n",
    "with open(f'{global_path}/{set_id}/perfmatrix3-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix3, f)\n",
    "with open(f'{global_path}/{set_id}/perfmatrix4-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix4, f)\n",
    "with open(f'{global_path}/{set_id}/perfmatrix5-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix5, f)\n",
    "with open(f'{global_path}/{set_id}/perfmatrix6-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]})-sampl{samples}.pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix5, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Performance Matrix\n",
    "files = os.listdir(f'{global_path}/{set_id}/')\n",
    "perf_files = [f for f in files if fnmatch.fnmatch(f, 'perfmatrix*none*.pkl')]\n",
    "perf_widget = widgets.Dropdown(options=perf_files[:],value=perf_files[0],description='Performance Matrix :',disabled=False); perf_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the matrix from the file\n",
    "perf_string = perf_widget.value\n",
    "parts_string = perf_string .split('-')\n",
    "lr = 'learn-' + '-'.join(parts_string[1:])\n",
    "perp = 'perp-' + '-'.join(parts_string[1:])\n",
    "with open(f'{global_path}/{set_id}/{perf_string }', 'rb') as f:\n",
    "    performance_matrix = pickle.load(f)\n",
    "with open(f'{global_path}/{set_id}/{lr}', 'rb') as f:\n",
    "    learning_rates = pickle.load(f)\n",
    "with open(f'{global_path}/{set_id}/{perp}', 'rb') as f:\n",
    "    perplexities = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Define Font Settings\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rcParams['font.monospace'] = \"Courier\"\n",
    "plt.rcParams[\"font.family\"] = \"monospace\"\n",
    "\n",
    "im = ax.imshow(performance_matrix, cmap='Spectral')\n",
    "ax.set_yticks(np.arange(len(learning_rates)),labels = learning_rates)\n",
    "ax.set_ylabel('learning_rate')\n",
    "ax.set_xticks(np.arange(len(perplexities)), labels = perplexities)\n",
    "ax.set_xlabel('perplexity')\n",
    "ax.grid(False) \n",
    "\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"performance_metric\")\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(perplexities)):\n",
    "        text = ax.text(j, i, round(performance_matrix[i, j],2),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE Setting\n",
    "n_comp = 2\n",
    "perp = 1280\n",
    "learn_rate = 400\n",
    "early_exag = 1\n",
    "iterations = 2000\n",
    "\n",
    "# Define TSNE Model\n",
    "tsne_model = TSNE(n_components = n_comp , perplexity = perp, learning_rate = learn_rate, n_iter = iterations, early_exaggeration = early_exag, init='random')\n",
    "# PCA\n",
    "pca_model = PCA(21)\n",
    "X_new = pca_model.fit_transform(features) \n",
    "# Run TSNE Model on Data\n",
    "tsne_out = tsne_model.fit_transform(X_new)\n",
    "print(tsne_out.shape)\n",
    "\n",
    "# Save TSNE Output\n",
    "df_tsne = pd.DataFrame(tsne_out, columns=['tsne1', 'tsne2'])\n",
    "df_tsne['obsreg_id'] = list(ids)\n",
    "input_file = input_widget.value\n",
    "data_rep = input_file.split(\".pkl\")[0]\n",
    "df_tsne.to_csv(f'{global_path}/{set_id}/tsne-{set_id}-{data_rep}-{n_comp}D-perp{perp}-lr{learn_rate}-ee{early_exag}-it{iterations}.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f'{global_path}/{set_id}/')\n",
    "tsne_files = [f for f in files if fnmatch.fnmatch(f, 'tsne*nE16*')]\n",
    "tsne_widget = widgets.Dropdown(options=tsne_files[:],value=tsne_files[0],description='TSNE File :',disabled=False); tsne_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "tsne_file = tsne_widget.value\n",
    "df_tsne = pd.read_csv(f'{global_path}/{set_id}/{tsne_file}')\n",
    "# Use loc to select rows that match the IDs in list2, and store the result in a new DataFrame\n",
    "flares = ['9109_333','9109_344','13637_1078','14368_489','14368_503','14431_16','14542_18','10822_185','10955_21','10996_5','2833_53','13610_112','15214_29']\n",
    "dips = ['10783_10','10871_10','11059_10','9070_10','9072_10','13814_567','13682_9','1708_192','1708_193','1712_91','15553_237','13681_9','13813_86']\n",
    "rosanne = ['13814_567']\n",
    "others = ['10346_11','10542_331','10542_331','10545_496','10556_5752','10556_6687','10811_223','10821_241', '1878_331','10930_1050','10953_275','10956_64']\n",
    "tsne_flares = df_tsne.loc[df_tsne['obsreg_id'].isin(flares)]\n",
    "tsne_dips = df_tsne.loc[df_tsne['obsreg_id'].isin(dips)]\n",
    "tsne_rosanne = df_tsne.loc[df_tsne['obsreg_id'].isin(rosanne)]\n",
    "tsne_others = df_tsne.loc[df_tsne['obsreg_id'].isin(others)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 12),constrained_layout = True)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rcParams['font.monospace'] = \"Courier\"\n",
    "plt.rcParams[\"font.family\"] = \"monospace\"\n",
    "fig.suptitle(tsne_file)\n",
    "colourmap_hard = 'viridis_r'\n",
    "colourmap_var = 'plasma_r'\n",
    "\n",
    "hard_hm = axs[0,0].scatter(df_tsne['tsne1'], df_tsne['tsne2'],c=df_properties['hard_hm'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,0].set_xlabel('tSNE feature 1')\n",
    "axs[0,0].set_ylabel('tSNE feature 2')\n",
    "axs[0,0].set_title(f'hard_hm')\n",
    "fig.colorbar(hard_hm, ax = axs[0,0])\n",
    "\n",
    "hard_hs = axs[0,1].scatter(df_tsne['tsne1'], df_tsne['tsne2'], c=df_properties['hard_hs'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,1].set_xlabel('tSNE feature 1')\n",
    "axs[0,1].set_ylabel('tSNE feature 2')\n",
    "axs[0,1].set_title(f'hard_hs')\n",
    "fig.colorbar(hard_hs, ax = axs[0,1])\n",
    "\n",
    "hard_ms = axs[0,2].scatter(df_tsne['tsne1'], df_tsne['tsne2'], c=df_properties['hard_ms'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,2].set_xlabel('tSNE feature 1')\n",
    "axs[0,2].set_ylabel('tSNE feature 2')\n",
    "axs[0,2].set_title(f'hard_ms')\n",
    "fig.colorbar(hard_ms, ax = axs[0,2])\n",
    "\n",
    "var_h = axs[1,0].scatter(df_tsne['tsne1'], df_tsne['tsne2'], c=df_properties['var_prob_h'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,0].set_xlabel('tSNE feature 1')\n",
    "axs[1,0].set_ylabel('tSNE feature 2')\n",
    "axs[1,0].set_title(f'var_prob_h')\n",
    "fig.colorbar(var_h, ax = axs[1,0])\n",
    "\n",
    "var_m = axs[1,1].scatter(df_tsne['tsne1'], df_tsne['tsne2'], c=df_properties['var_prob_m'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,1].set_xlabel('tSNE feature 1')\n",
    "axs[1,1].set_ylabel('tSNE feature 2')\n",
    "axs[1,1].set_title(f'var_prob_m')\n",
    "fig.colorbar(var_m, ax = axs[1,1])\n",
    "\n",
    "var_s = axs[1,2].scatter(df_tsne['tsne1'], df_tsne['tsne2'], c=df_properties['var_prob_s'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,2].set_xlabel('tSNE feature 1')\n",
    "axs[1,2].set_ylabel('tSNE feature 2')\n",
    "axs[1,2].set_title(f'var_prob_s')\n",
    "fig.colorbar(var_s, ax = axs[1,2])\n",
    "\n",
    "for ax in axs.flatten()[0:1]:\n",
    "    ax.scatter(tsne_flares['tsne1'], tsne_flares['tsne2'], c='red', marker='x', s=100, label='Flares')\n",
    "    ax.scatter(tsne_dips['tsne1'], tsne_dips['tsne2'], c='blue', marker='x', s=100, label='Dips')\n",
    "    ax.scatter(tsne_rosanne['tsne1'], tsne_rosanne['tsne2'], c='black', marker='x', s=100, label='Rosanne')\n",
    "    ax.scatter(tsne_others['tsne1'], tsne_others['tsne2'], c='black', marker='x', s=100, label='Others')\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. UMAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca Setting\n",
    "n_comp = 2\n",
    "neighbours = 50\n",
    "min_d = 0.1\n",
    "learn_rate = 0.1\n",
    "sp = 2\n",
    "\n",
    "# Define pca Model\n",
    "umap_model = umap.UMAP(n_components = n_comp, n_neighbors = neighbours, min_dist = min_d, learning_rate = learn_rate, random_state=42, spread = sp)\n",
    "# Run pca Model on Data\n",
    "umap_out = umap_model.fit_transform(features)\n",
    "print(umap_out.shape)\n",
    "\n",
    "# Save pca Output\n",
    "df_umap = pd.DataFrame(umap_out, columns=['umap1', 'umap2'])\n",
    "df_umap['obsreg_id'] = list(ids)\n",
    "input_file = input_widget.value\n",
    "data_rep = input_file.split(\".pkl\")[0]\n",
    "df_umap.to_csv(f'{global_path}/{set_id}/umap-{set_id}-{data_rep}-{n_comp}D-neighb{neighbours}-mind-{min_d}-lr{learn_rate}-sp{sp}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f'{global_path}/{set_id}/')\n",
    "umap_files = [f for f in files if fnmatch.fnmatch(f, 'umap*')]\n",
    "umap_widget = widgets.Dropdown(options=umap_files[:],value=umap_files[0],description='UMAP File :',disabled=False); umap_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "umap_file = umap_widget.value\n",
    "df_umap = pd.read_csv(f'{global_path}/{set_id}/{umap_file}')\n",
    "# Use loc to select rows that match the IDs in list2, and store the result in a new DataFrame\n",
    "flares = ['9109_333','9109_344','13637_1078','14368_489','14368_503','14431_16','14542_18','10822_185','10955_21','10996_5','2833_53','13610_112','15214_29']\n",
    "dips = ['10783_10','10871_10','11059_10','9070_10','9072_10','13814_567','13682_9','1708_192','1708_193','1712_91','15553_237','13681_9','13813_86']\n",
    "rosanne = ['13814_567']\n",
    "umap_flares = df_umap.loc[df_umap['obsreg_id'].isin(flares)]\n",
    "umap_dips = df_umap.loc[df_umap['obsreg_id'].isin(dips)]\n",
    "umap_rosanne = df_umap.loc[df_umap['obsreg_id'].isin(rosanne)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 12),constrained_layout = True)\n",
    "fig.suptitle(umap_file)\n",
    "colourmap_hard = 'viridis_r' #inferno 'viridis_r' 'plasma_r'\n",
    "colourmap_var = 'plasma_r'\n",
    "\n",
    "hard_hm = axs[0,0].scatter(df_umap['umap1'], df_umap['umap2'],c=df_properties['hard_hm'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,0].set_xlabel('UMAP feature 1')\n",
    "axs[0,0].set_ylabel('UMAP feature 2')\n",
    "axs[0,0].set_title(f'hard_hm')\n",
    "fig.colorbar(hard_hm, ax = axs[0,0])\n",
    "\n",
    "hard_hs = axs[0,1].scatter(df_umap['umap1'], df_umap['umap2'], c=df_properties['hard_hs'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,1].set_xlabel('UMAP feature 1')\n",
    "axs[0,1].set_ylabel('UMAP feature 2')\n",
    "axs[0,1].set_title(f'hard_hs')\n",
    "fig.colorbar(hard_hs, ax = axs[0,1])\n",
    "\n",
    "hard_ms = axs[0,2].scatter(df_umap['umap1'], df_umap['umap2'], c=df_properties['hard_ms'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,2].set_xlabel('UMAP feature 1')\n",
    "axs[0,2].set_ylabel('UMAP feature 2')\n",
    "axs[0,2].set_title(f'hard_ms')\n",
    "fig.colorbar(hard_ms, ax = axs[0,2])\n",
    "\n",
    "var_h = axs[1,0].scatter(df_umap['umap1'], df_umap['umap2'], c=df_properties['var_prob_h'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,0].set_xlabel('UMAP feature 1')\n",
    "axs[1,0].set_ylabel('UMAP feature 2')\n",
    "axs[1,0].set_title(f'var_prob_h')\n",
    "fig.colorbar(var_h, ax = axs[1,0])\n",
    "\n",
    "var_m = axs[1,1].scatter(df_umap['umap1'], df_umap['umap2'], c=df_properties['var_prob_m'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,1].set_xlabel('UMAP feature 1')\n",
    "axs[1,1].set_ylabel('UMAP feature 2')\n",
    "axs[1,1].set_title(f'var_prob_m')\n",
    "fig.colorbar(var_m, ax = axs[1,1])\n",
    "\n",
    "var_s = axs[1,2].scatter(df_umap['umap1'], df_umap['umap2'], c=df_properties['var_prob_s'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,2].set_xlabel('UMAP feature 1')\n",
    "axs[1,2].set_ylabel('UMAP feature 2')\n",
    "axs[1,2].set_title(f'var_prob_s')\n",
    "fig.colorbar(var_s, ax = axs[1,2])\n",
    "\n",
    "for ax in axs.flatten()[0:1]:\n",
    "    ax.scatter(umap_flares['umap1'], umap_flares['umap2'], c='red', marker='x', s=100, label='Flares')\n",
    "    ax.scatter(umap_dips['umap1'], umap_dips['umap2'], c='blue', marker='x', s=100, label='Dips')\n",
    "    ax.scatter(umap_rosanne['umap1'], umap_rosanne['umap2'], c='black', marker='x', s=100, label='Rosanne')\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "pca_model = PCA(n_components=2)\n",
    "pca_out = pca_model.fit_transform(features) \n",
    "\n",
    "# Save UMAP Output\n",
    "df_pca = pd.DataFrame(pca_out, columns=['pca1', 'pca2'])\n",
    "df_pca['obsreg_id'] = list(ids)\n",
    "input_file = input_widget.value\n",
    "data_rep = input_file.split(\".pkl\")[0]\n",
    "df_pca.to_csv(f'{global_path}/{set_id}/pca-{set_id}-{data_rep}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f'{global_path}/{set_id}/')\n",
    "pca_files = [f for f in files if fnmatch.fnmatch(f, 'pca*')]\n",
    "pca_widget = widgets.Dropdown(options=pca_files[:],value=pca_files[0],description='PCA File :',disabled=False); pca_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "pca_file = pca_widget.value\n",
    "df_pca = pd.read_csv(f'{global_path}/{set_id}/{pca_file}')\n",
    "# Use loc to select rows that match the IDs in list2, and store the result in a new DataFrame\n",
    "flares = ['9109_333','9109_344','13637_1078','14368_489','14368_503','14431_16','14542_18','10822_185','10955_21','10996_5','2833_53','13610_112','15214_29']\n",
    "dips = ['10783_10','10871_10','11059_10','9070_10','9072_10','13814_567','13682_9','1708_192','1708_193','1712_91','15553_237','13681_9','13813_86']\n",
    "rosanne = ['13814_567']\n",
    "pca_flares = df_pca.loc[df_pca['obsreg_id'].isin(flares)]\n",
    "pca_dips = df_pca.loc[df_pca['obsreg_id'].isin(dips)]\n",
    "pca_rosanne = df_pca.loc[df_pca['obsreg_id'].isin(rosanne)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 12),constrained_layout = True)\n",
    "fig.suptitle(pca_file)\n",
    "colourmap_hard = 'viridis_r' #inferno 'viridis_r' 'plasma_r'\n",
    "colourmap_var = 'plasma_r'\n",
    "\n",
    "hard_hm = axs[0,0].scatter(df_pca['pca1'], df_pca['pca2'],c=df_properties['hard_hm'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,0].set_xlabel('pca feature 1')\n",
    "axs[0,0].set_ylabel('pca feature 2')\n",
    "axs[0,0].set_title(f'hard_hm')\n",
    "fig.colorbar(hard_hm, ax = axs[0,0])\n",
    "\n",
    "hard_hs = axs[0,1].scatter(df_pca['pca1'], df_pca['pca2'], c=df_properties['hard_hs'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,1].set_xlabel('pca feature 1')\n",
    "axs[0,1].set_ylabel('pca feature 2')\n",
    "axs[0,1].set_title(f'hard_hs')\n",
    "fig.colorbar(hard_hs, ax = axs[0,1])\n",
    "\n",
    "hard_ms = axs[0,2].scatter(df_pca['pca1'], df_pca['pca2'], c=df_properties['hard_ms'], s=0.1, cmap=colourmap_hard)\n",
    "axs[0,2].set_xlabel('pca feature 1')\n",
    "axs[0,2].set_ylabel('pca feature 2')\n",
    "axs[0,2].set_title(f'hard_ms')\n",
    "fig.colorbar(hard_ms, ax = axs[0,2])\n",
    "\n",
    "var_h = axs[1,0].scatter(df_pca['pca1'], df_pca['pca2'], c=df_properties['var_prob_h'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,0].set_xlabel('pca feature 1')\n",
    "axs[1,0].set_ylabel('pca feature 2')\n",
    "axs[1,0].set_title(f'var_prob_h')\n",
    "fig.colorbar(var_h, ax = axs[1,0])\n",
    "\n",
    "var_m = axs[1,1].scatter(df_pca['pca1'], df_pca['pca2'], c=df_properties['var_prob_m'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,1].set_xlabel('pca feature 1')\n",
    "axs[1,1].set_ylabel('pca feature 2')\n",
    "axs[1,1].set_title(f'var_prob_m')\n",
    "fig.colorbar(var_m, ax = axs[1,1])\n",
    "\n",
    "var_s = axs[1,2].scatter(df_pca['pca1'], df_pca['pca2'], c=df_properties['var_prob_s'], s=0.1, cmap=colourmap_var)\n",
    "axs[1,2].set_xlabel('pca feature 1')\n",
    "axs[1,2].set_ylabel('pca feature 2')\n",
    "axs[1,2].set_title(f'var_prob_s')\n",
    "fig.colorbar(var_s, ax = axs[1,2])\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.scatter(pca_flares['pca1'], pca_flares['pca2'], c='red', marker='x', s=100, label='Flares')\n",
    "    ax.scatter(pca_dips['pca1'], pca_dips['pca2'], c='blue', marker='x', s=100, label='Dips')\n",
    "    ax.scatter(pca_rosanne['pca1'], pca_rosanne['pca2'], c='black', marker='x', s=100, label='Rosanne')\n",
    "    ax.set_xlim([-25,50])\n",
    "    ax.set_ylim([-50,25])\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "X = X[0:5000,:]\n",
    "Y = Y[0:X.shape[0],:]\n",
    "\n",
    "# define the list of learning rates and perplexities to try\n",
    "learning_rates = [10, 25, 50, 75, 100, 250, 500]\n",
    "perplexities = [5, 10, 20, 30, 40, 50,75,100]\n",
    "performance_matrix = np.zeros((len(learning_rates), len(perplexities)))\n",
    "total_count = len(learning_rates) * len(perplexities)\n",
    "counter = 0\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, perp in enumerate(perplexities):\n",
    "\n",
    "        # Compute Xtsne embedding\n",
    "        tsne = TSNE(learning_rate=lr, perplexity=perp, n_iter = 2000, early_exaggeration = 1, init='random')\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "        # compute pairwise distances\n",
    "        dY = pdist(Y)\n",
    "        dY = squareform(dY)\n",
    "        #dY = scaler.fit_transform(dY)\n",
    "\n",
    "        # # compute pairwise similarities between labels\n",
    "        # label_similarities = np.zeros((Y.shape[0], Y.shape[0]))\n",
    "        # for i in range(Y.shape[0]):\n",
    "        #     for j in range(i+1, Y.shape[0]):\n",
    "        #         correlation = np.corrcoef(Y[i,:], Y[j,:])[0,1]\n",
    "        #         label_similarities[i,j] = correlation\n",
    "        #         label_similarities[j,i] = correlation\n",
    "\n",
    "        # # compute the pairwise similarity matrix for the labels JACCARD SIMILARITY\n",
    "        # label_similarities2 = np.zeros((Y.shape[1], Y.shape[1]))\n",
    "        # for i in range(Y.shape[0]):\n",
    "        #     for j in range(i+1, Y.shape[0]):\n",
    "        #         similarity = np.sum(Y[:,i] & Y[:,j]) / np.sum(Y[:,i] | Y[:,j])\n",
    "        #         label_similarities[i,j] = similarity\n",
    "        #         label_similarities[j,i] = similarity\n",
    "\n",
    "        # compute pairwise distances\n",
    "        dX = pdist(X_tsne)\n",
    "        dX = squareform(dX)\n",
    "        #dX = scaler.fit_transform(dX)\n",
    "\n",
    "        # # compute pairwise similarities between labels\n",
    "        # tsne_similarities = np.zeros((X_tsne.shape[0], X_tsne.shape[0]))\n",
    "        # for i in range(X_tsne.shape[0]):\n",
    "        #     for j in range(i+1, X_tsne.shape[0]):\n",
    "        #         distance = np.linalg.norm(X_tsne[i,:]-X_tsne[j,:])\n",
    "        #         tsne_similarities[i,j] = distance\n",
    "        #         tsne_similarities[j,i] = distance\n",
    "\n",
    "        print(dY.shape,dX.shape)\n",
    "        # print(label_similarities.shape,tsne_similarities.shape)\n",
    "        performance_metric = spearmanr(dY.flatten(), dX.flatten())[0]\n",
    "        # performance_metric2 = spearmanr(label_similarities.flatten(), tsne_similarities.flatten())[0]\n",
    "        performance_matrix[i,j] = performance_metric \n",
    "print(performance_matrix)\n",
    "\n",
    "# save the matrix to a file\n",
    "input_file = input_widget.value\n",
    "data_rep = input_file.split(\".pkl\")[0]\n",
    "with open(f'{global_path}/{set_id}/perfmatrix-{set_id}-{data_rep}-lr({learning_rates[0]},{learning_rates[-1]})-pp({perplexities[0]},{perplexities[-1]}).pkl', 'wb') as f:\n",
    "    pickle.dump(performance_matrix, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciao-4.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa0ed6ea5fdefd83fab7eb4cb6966f67b14a46d682529109a8514cc91561ea32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
