{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/steven/Desktop/CSCData/23022need/acisf23022_000N027_r0122_regevt6_background_background.fits\n",
      "/Users/steven/Desktop/CSCData/23022need/acisf23022_000N020_r0122_reg6.fits\n",
      "/Users/steven/Desktop/CSCData/23022need/acisf23022_000N020_r0122_bkg6.fits\n",
      "2881.232771688578\n",
      "95312.0\n",
      "33.08028457004581\n"
     ]
    }
   ],
   "source": [
    "from crypt import methods\n",
    "from logging.handlers import BaseRotatingHandler\n",
    "from pyexpat import ErrorString\n",
    "from tkinter import N\n",
    "from tokenize import PlainToken\n",
    "from ciao_contrib.runtool import *\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import sys\n",
    "from re import sub\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bootstrap\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import entropy\n",
    "from astropy.stats import bayesian_blocks\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from matplotlib import axes\n",
    "from astropy.stats import histogram\n",
    "from statistics import mean, median, median_high\n",
    "from scipy.stats import norm\n",
    "from scipy import interpolate\n",
    "from numpy import trapz\n",
    "import astropy.stats as astats\n",
    "from region import *\n",
    "\n",
    "BEHR_DIR = '/Users/steven/BEHR/BEHR/'\n",
    "DATA_DIR = '/Users/steven/Desktop/CSCData/23022need/'\n",
    "evt_file = glob.glob(f'{DATA_DIR}*evt6*')[0]\n",
    "regevt_file = glob.glob(f'{DATA_DIR}*evt6_filtered*')[0]\n",
    "src_region = glob.glob(f'{DATA_DIR}*reg6*')[0]\n",
    "bkg_region = glob.glob(f'{DATA_DIR}*bkg6*')[0]\n",
    "bkgevt_file = glob.glob(f'{DATA_DIR}*evt6_back*')[0]\n",
    "\n",
    "# filtered_filename = evt_file.replace(\".fits\", \"_background.fits\")\n",
    "\n",
    "# dmcopy(f'{evt_file}[sky=region({bkg_region})]', filtered_filename)\n",
    "\n",
    "a=1\n",
    "\n",
    "print(evt_file )\n",
    "print(src_region)\n",
    "print(bkg_region)\n",
    "\n",
    "src_a = CXCRegion(f'region({src_region})')\n",
    "bkg_a = CXCRegion(f'region({bkg_region})')\n",
    "print(src_a.area())\n",
    "print(bkg_a.area())\n",
    "a_ratio = bkg_a.area()/src_a.area()\n",
    "print(a_ratio)\n",
    "\n",
    "\n",
    "def behr_hr(area,conf = '68.00'):\n",
    "    BEHR_DIR = '/Users/steven/BBBB/BEHR/'\n",
    "    DATA_DIR = '/Users/steven/Desktop/CSCData/23022need/'\n",
    "    evt_file = glob.glob(f'{DATA_DIR}*evt6*')[0]\n",
    "    regevt = glob.glob(f'{DATA_DIR}*evt6_filtered*')[0]\n",
    "    src_region = glob.glob(f'{DATA_DIR}*reg6*')[0]\n",
    "    bkg_region = glob.glob(f'{DATA_DIR}*bkg5*')[0]\n",
    "    bevt = glob.glob(f'{DATA_DIR}*evt6_back*')[0]\n",
    "\n",
    "    with fits.open(regevt) as hdul:\n",
    "        # Events dataframe\n",
    "        events = hdul[\"Events\"].data\n",
    "        events_table = Table(events)\n",
    "        events_cols = events.columns.names\n",
    "        df_events = pd.DataFrame.from_records(events_table, columns=events_cols)\n",
    "        df_events = df_events.sort_values(by=[\"time\"])\n",
    "        # GTI (Good Time Interval) dataframe\n",
    "        gti = hdul[\"GTI\"].data\n",
    "        gti_table = Table(gti)\n",
    "        gti_cols = gti.columns.names\n",
    "        df_gti = pd.DataFrame.from_records(gti_table, columns=gti_cols)\n",
    "        # Apply GTI filter to events\n",
    "        gti_mask = np.zeros(len(df_events), dtype=bool)\n",
    "        for i in range(len(df_gti)):\n",
    "            start = df_gti.iloc[i]['START']\n",
    "            stop = df_gti.iloc[i]['STOP']\n",
    "            gti_mask |= (df_events[\"time\"] >= start) & (df_events[\"time\"] < stop)\n",
    "        df_events = df_events[gti_mask]\n",
    "        # Apply energy, pha, grade filters to events\n",
    "        df_events = df_events[(df_events['pha']>40) & (df_events['grade']>=0) & (df_events['energy']>500) & (df_events['energy']<7000)]\n",
    "        start_time = min(df_events.time.values)\n",
    "        src_times = df_events.time.values - start_time\n",
    "        src_energies = df_events.energy.values\n",
    "\n",
    "    with fits.open(bevt) as hdul:\n",
    "        # Events dataframe\n",
    "        bevents = hdul[\"Events\"].data\n",
    "        bevents_table = Table(bevents)\n",
    "        bevents_cols = bevents.columns.names\n",
    "        df_bevents = pd.DataFrame.from_records(bevents_table, columns=bevents_cols)\n",
    "        df_bevents = df_bevents.sort_values(by=[\"time\"])\n",
    "        # GTI (Good Time Interval) dataframe\n",
    "        bgti = hdul[\"GTI\"].data\n",
    "        bgti_table = Table(bgti)\n",
    "        bgti_cols = bgti.columns.names\n",
    "        df_bgti = pd.DataFrame.from_records(bgti_table, columns=bgti_cols)\n",
    "        # Apply GTI filter to events\n",
    "        bgti_mask = np.zeros(len(df_bevents), dtype=bool)\n",
    "        for i in range(len(df_bgti)):\n",
    "            start = df_bgti.iloc[i]['START']\n",
    "            stop = df_bgti.iloc[i]['STOP']\n",
    "            bgti_mask |= (df_bevents[\"time\"] >= start) & (df_bevents[\"time\"] < stop)\n",
    "        df_bevents = df_bevents[bgti_mask]\n",
    "        # Apply energy, pha, grade filters to events\n",
    "        df_bevents = df_bevents[(df_bevents['pha']>40) & (df_bevents['grade']>=0) & (df_bevents['energy']>500) & (df_bevents['energy']<7000)]\n",
    "        bkg_times = df_bevents.time.values - start_time\n",
    "        bkg_energies = df_bevents.energy.values\n",
    "\n",
    "    bb_bins = astats.bayesian_blocks(src_times, fitness='events',p0 = 0.01) # p0 = 0.1\n",
    "    bin_widths = bb_bins[1:] - bb_bins[:-1]\n",
    "    counts, _ = np.histogram(src_times, bins=bb_bins)\n",
    "\n",
    "    times_and_energies = np.column_stack((src_times,src_energies))\n",
    "    bkg_te = np.column_stack((bkg_times,bkg_energies))\n",
    "    # define soft/medium/hard bands\n",
    "    soft_filter = times_and_energies[:,1] < 1200\n",
    "    medium_filter = (times_and_energies[:,1] < 2000) *(times_and_energies[:,1] > 1200)\n",
    "    hard_filter = times_and_energies[:,1] > 2000\n",
    "    soft_medium_filter = times_and_energies[:,1] < 2000\n",
    "    soft_filter_b = bkg_te[:,1] < 1200\n",
    "    medium_filter_b = (bkg_te[:,1] < 2000) *(bkg_te[:,1] > 1200)\n",
    "    hard_filter_b = bkg_te[:,1] > 2000\n",
    "    soft_medium_filter_b = bkg_te[:,1] < 2000\n",
    "    # generate data for each band\n",
    "    soft_lc = times_and_energies[soft_filter]\n",
    "    medium_lc = times_and_energies[medium_filter]\n",
    "    hard_lc =times_and_energies[hard_filter]\n",
    "    sm_lc = times_and_energies[soft_medium_filter]\n",
    "    soft_b = bkg_te[soft_filter_b]\n",
    "    medium_b = bkg_te[medium_filter_b]\n",
    "    hard_b = bkg_te[hard_filter_b]\n",
    "    sm_b = bkg_te[soft_medium_filter_b]\n",
    "    #counts\n",
    "    counts_s, _ = np.histogram(soft_lc[:,0], bins=   bb_bins)\n",
    "    counts_m, _ = np.histogram(medium_lc[:,0], bins=   bb_bins)\n",
    "    counts_h, _ = np.histogram(hard_lc[:,0], bins=   bb_bins)\n",
    "    counts_sm, _ = np.histogram(sm_lc[:,0], bins=   bb_bins)\n",
    "    counts_s_b, _ = np.histogram(soft_b[:,0], bins=   bb_bins)\n",
    "    counts_m_b, _ = np.histogram(medium_b[:,0], bins=   bb_bins)\n",
    "    counts_h_b, _ = np.histogram(hard_b[:,0], bins=   bb_bins)\n",
    "    counts_sm_b, _ = np.histogram(sm_b[:,0], bins=   bb_bins)\n",
    "\n",
    "    outfile = f'{DATA_DIR}/behr' \n",
    "    for i in range(len(counts_s)):\n",
    "        with open(outfile,'w') as writeto:\n",
    "            writeto.write(f'cd {BEHR_DIR}')\n",
    "            writeto.write(f'\\n echo \"softsrc={counts_sm[i]} hardsrc={counts_h[i]}   softbkg={counts_sm_b[i]}   hardbkg={counts_h_b[i]} softarea={area} outputPr=True algo=quad\"')\n",
    "            writeto.write(f'\\n./BEHR softsrc={counts_s[i]} hardsrc={counts_h[i]}   softbkg={counts_sm_b[i]}   hardbkg={counts_h_b[i]}  softarea={area} output={BEHR_DIR}/{i}_BEHRresults level={conf}')\n",
    "\n",
    "    subprocess.run(f'bash {outfile}', shell = True)\n",
    "\n",
    "\n",
    "    return counts_sm, counts_sm_b\n",
    "\n",
    "# sm, smb = behr_hr(20,conf = '68.00')\n",
    "\n",
    "\n",
    "# def sig_make_behr_running_avg(evt,srcreg,bkgreg,BEHR_DIR,outfile,BEHR_outdir,N,confidence='68.00'):\n",
    "#     soft_src,hard_src,soft_bkg,hard_bkg = constant_count_split(evt,srcreg,bkgreg,2000,N)\n",
    "\n",
    "#     hard_area = region_area(evt,bkgreg,3000)/region_area(evt,srcreg,3000)\n",
    "#     soft_area = region_area(evt,bkgreg,1000)/region_area(evt,srcreg,1000)\n",
    "    \n",
    "#     ## need list of 2N+1 \n",
    "    \n",
    "\n",
    "#     with open(outfile,'w') as writeto:\n",
    "#         writeto.write(f'cd {BEHR_DIR}')\n",
    "#         for i in range(len(soft_src)):\n",
    "#             writeto.write(f'\\n echo \"softsrc={soft_src[i]} hardsrc={hard_src[i]}   softbkg={soft_bkg[i]}   hardbkg={hard_bkg[i]}\"')\n",
    "#             writeto.write(f'\\n./BEHR softsrc={soft_src[i]} hardsrc={hard_src[i]}   softbkg={soft_bkg[i]}   hardbkg={hard_bkg[i]}   softarea={soft_area} hardarea={hard_area} output={BEHR_outdir}/{i}_BEHRresults level={confidence}')\n",
    "\n",
    "\n",
    "#     def sig_make_behr_running_avg(evt,srcreg,bkgreg,BEHR_DIR,outfile,BEHR_outdir,N,confidence='68.00'):\n",
    "\n",
    "#     soft_src,hard_src,soft_bkg,hard_bkg = constant_count_split(evt,srcreg,bkgreg,2000,N)\n",
    "\n",
    "#     hard_area = region_area(evt,bkgreg,3000)/region_area(evt,srcreg,3000)\n",
    "#     soft_area = region_area(evt,bkgreg,1000)/region_area(evt,srcreg,1000)\n",
    "    \n",
    "#     ## need list of 2N+1 \n",
    "    \n",
    "\n",
    "#     with open(outfile,'w') as writeto:\n",
    "#         writeto.write(f'cd {BEHR_DIR}')\n",
    "#         for i in range(len(soft_src)):\n",
    "#             writeto.write(f'\\n echo \"softsrc={soft_src[i]} hardsrc={hard_src[i]}   softbkg={soft_bkg[i]}   hardbkg={hard_bkg[i]}\"')\n",
    "#             writeto.write(f'\\n./BEHR softsrc={soft_src[i]} hardsrc={hard_src[i]}   softbkg={soft_bkg[i]}   hardbkg={hard_bkg[i]}   softarea={soft_area} hardarea={hard_area} output={BEHR_outdir}/{i}_BEHRresults level={confidence}')\n",
    "\n",
    "    \n",
    "\n",
    "# def sig_run_BEHR(bash_file):\n",
    "#     subprocess.run(f'bash {bash_file}', shell = True)\n",
    "\n",
    "\n",
    "    # bb_s = astats.bayesian_blocks(soft_lc[:,0], fitness='events',p0 = 0.01) # p0 = 0.\n",
    "    # counts_s, _ = np.histogram(soft_lc[:,0], bins=   bb_bins)\n",
    "    # bb_m = astats.bayesian_blocks(medium_lc[:,0], fitness='events',p0 = 0.01) # p0 = 0.\n",
    "    # counts_m, _ = np.histogram(medium_lc[:,0], bins=   bb_bins)\n",
    "    # bb_h = astats.bayesian_blocks(hard_lc[:,0], fitness='events',p0 = 0.01) # p0 = 0.\n",
    "    # counts_h, _ = np.histogram(hard_lc[:,0], bins=   bb_bins)\n",
    "    # bb_sm = astats.bayesian_blocks(sm_lc[:,0], fitness='events',p0 = 0.01) # p0 = 0.\n",
    "    # counts_sm, _ = np.histogram(sm_lc[:,0], bins=  bb_bins)\n",
    "    # # # union them, and remove outliers\n",
    "    # edges_union = np.concatenate((bb_s,bb_m,bb_h))\n",
    "    # sorted_edges_union = np.sort(edges_union)\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    # ax.fill_between(bb_bins, np.append(counts, counts[-1]), step=\"post\", color = 'red', alpha=0.4, label='Bayesian Blocks Counts')\n",
    "    # ax.tick_params(direction='in', top=True)\n",
    "    # ax.minorticks_on()\n",
    "    # ax.set_ylim([0,max(counts)*1.1])\n",
    "    # ax.yaxis.label.set_color('red')\n",
    "    # ax.tick_params(axis='y', colors='red')\n",
    "    # ax.set_ylabel('Counts')\n",
    "    # countrate = counts/bin_widths \n",
    "    # bin_centers = (bb_bins[:-1] + bb_bins[1:]) / 2\n",
    "    # ax2 = ax.twinx()\n",
    "    # rate = ax2.step(bb_bins, np.append(countrate, countrate[-1]), where='post', color='blue',label = 'Bayesian Blocks Count Rate')\n",
    "    # ax2.minorticks_on()\n",
    "    # ax2.set_ylabel('Count Rate [1/s]')\n",
    "    # ax2.yaxis.label.set_color('blue')\n",
    "    # ax2.tick_params(axis='y', colors='blue')\n",
    "    # ax2.set_ylim([0,max(countrate)*1.1])\n",
    "    # ax2.tick_params(direction='in', top=True)\n",
    "    # # Combine all labels in one legend\n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # handles += rate\n",
    "    # labels += ['Bayesian Blocks Count Rate']\n",
    "    # plt.legend(handles=handles, labels=labels, loc='upper left', frameon=False, fontsize = 14)\n",
    "    # plt.plot()\n",
    "\n",
    "\n",
    "\n",
    "#  with fits.open(filename) as hdul:\n",
    "#             # Events dataframe\n",
    "#             events = hdul[\"Events\"].data\n",
    "#             events_table = Table(events)\n",
    "#             events_cols = events.columns.names\n",
    "#             df_events = pd.DataFrame.from_records(events_table, columns=events_cols)\n",
    "#             df_events = df_events.sort_values(by=[\"time\"])\n",
    "#             # GTI (Good Time Interval) dataframe\n",
    "#             gti = hdul[\"GTI\"].data\n",
    "#             gti_table = Table(gti)\n",
    "#             gti_cols = gti.columns.names\n",
    "#             df_gti = pd.DataFrame.from_records(gti_table, columns=gti_cols)\n",
    "#             # Apply GTI filter to events\n",
    "#             gti_mask = np.zeros(len(df_events), dtype=bool)\n",
    "#             for i in range(len(df_gti)):\n",
    "#                 start = df_gti.iloc[i]['START']\n",
    "#                 stop = df_gti.iloc[i]['STOP']\n",
    "#                 gti_mask |= (df_events[\"time\"] >= start) & (df_events[\"time\"] < stop)\n",
    "#             df_events = df_events[gti_mask]\n",
    "#             # Apply energy, pha, grade filters to events\n",
    "#             df_events = df_events[(df_events['pha']>40) & (df_events['grade']>=0) & (df_events['energy']>500) & (df_events['energy']<7000)]\n",
    "#             # Add obsid and region_id column (from filename)\n",
    "#             try: \n",
    "#                 df_events[\"obsid\"] = int(filename.split('_')[-5][-5:])\n",
    "#             except: \n",
    "#                 df_events[\"obsid\"] = int(filename.split('_')[-6][-5:])\n",
    "#             df_events[\"region_id\"] = int(filename.split('_')[-3][-4:])\n",
    "#             # Append to dataframe list\n",
    "#             list_df_events.append(df_events)\n",
    "#             counter = counter+1\n",
    "#             clear_output(wait=True)\n",
    "#             print(f'Progress: {counter}/{total}')\n",
    "\n",
    "#  region_filename = [region for region in glob.iglob(f'{global_path}/{set_id}/acisf*reg3.fits.gz') if str(obsid) in region and str(regionid) in region][0]\n",
    "#         filtered_filename = event_filename.replace(\".fits\", \"_filtered.fits\")\n",
    "#         print('Event Filename: ', event_filename)\n",
    "#         print('Region Filename: ', region_filename)\n",
    "#         filtered_filename = event_filename.replace(\".fits\", \"_filtered.fits\")\n",
    "#         try:\n",
    "#             ciao_contrib.runtool.dmcopy(f'{event_filename}[sky=region({region_filename})]', filtered_filename)\n",
    "#             print('Filtered Event Filename: ', filtered_filename)\n",
    "#         except OSError: \n",
    "#             print(f'{filtered_filename} already exists!')\n",
    "#         counter = counter+1\n",
    "#         clear_output(wait=True)\n",
    "#         print(f'Progress: {counter}/{total}')\n",
    "#['time', 'ccd_id', 'node_id', 'expno', 'chipx', 'chipy', 'tdetx', 'tdety', 'detx', 'dety', 'x', 'y', 'pha', 'pha_ro', 'energy', 'pi', 'fltgrade', 'grade', 'status']\n",
    "\n",
    "\n",
    "    # #getting events (time+energy) from src_region\n",
    "    # dmlist.punlearn()\n",
    "    # dmlist.infile = f'{evt}[sky=region({src_region})][cols time, energy]'\n",
    "    # dmlist.opt = 'data,clean'\n",
    "    # out = dmlist().split('\\n')\n",
    "    # out_np = np.genfromtxt(out,dtype='str').astype('float64')\n",
    "    # start_time = out_np[0,0]\n",
    "    # src_times = out_np[::,0] - start_time\n",
    "    # src_energies = out_np[::,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def sig_make_behr_file(BEHR_DIR,soft_src,hard_src,soft_bkg,hard_bkg,area,outfile,BEHR_outdir):\n",
    "    with open(outfile,'w') as writeto:\n",
    "        writeto.write(f'cd {BEHR_DIR}')\n",
    "        writeto.write(f'\\n echo \"softsrc={soft_src} hardsrc={hard_src} softbkg={soft_bkg} hardbkg={hard_bkg} softarea={area} outputPr=True algo=quad\"')\n",
    "        writeto.write(f'\\n./BEHR softsrc={soft_src} hardsrc={hard_src} softbkg={soft_bkg} hardbkg={hard_bkg} softarea={area} outputPr=true algo=quad output={BEHR_outdir}/BEHRresults')\n",
    "    \n",
    "    \n",
    "    print('Running quad BEHR...')\n",
    "\n",
    "    sig_run_BEHR(outfile)\n",
    "    \n",
    "    print(\"Reading BEHR...\")\n",
    "    \n",
    "    data = np.loadtxt(f'{BEHR_outdir}/BEHRresults_prob.txt')\n",
    "    c_value = data[::,4]\n",
    "    prob = data[::,5]\n",
    "    # plt.scatter(c_value,prob)\n",
    "    # plt.show()\n",
    "    \n",
    "    return list(zip(c_value, prob))\n",
    "\n",
    "def sig_make_behr_file_gibbs(BEHR_DIR,soft_src,hard_src,soft_bkg,hard_bkg,area,outfile,BEHR_outdir):\n",
    "    with open(outfile,'w') as writeto:\n",
    "        writeto.write(f'cd {BEHR_DIR}')\n",
    "        writeto.write(f'\\n echo \"softsrc={soft_src} hardsrc={hard_src} softbkg={soft_bkg} hardbkg={hard_bkg} softarea={area} nsim=40000 algo=gibbs outputMC=True\"')\n",
    "        writeto.write(f'\\n./BEHR softsrc={soft_src} hardsrc={hard_src} softbkg={soft_bkg} hardbkg={hard_bkg} softarea={area} nsim=40000 algo=gibbs outputMC=true output={BEHR_outdir}/BEHRresults')\n",
    "\n",
    "    print('Running gibbs BEHR...')\n",
    "\n",
    "    sig_run_BEHR(outfile)\n",
    "        \n",
    "    print(\"Reading BEHR...\")\n",
    "    \n",
    "    data = np.loadtxt(f'{BEHR_outdir}/BEHRresults_draws.txt')\n",
    "    softs = data[::,0]\n",
    "    hards = data[::,1]\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(0,len(hards)):\n",
    "        val = np.log10(softs[i]/hards[i])\n",
    "        vals.append(val)    \n",
    "    \n",
    "    sample_mean = np.mean(vals)\n",
    "    sample_std = np.std(vals)\n",
    "    low = min(vals)\n",
    "    high = max(vals)\n",
    "    print('Mean=%.3f, Standard Deviation=%.3f' % (sample_mean, sample_std))\n",
    "    dist = norm(sample_mean, sample_std)\n",
    "    x = np.linspace(low,high,100)\n",
    "    probabilities = [dist.pdf(value) for value in x]\n",
    "    # plt.hist(vals, bins=100, density=True)\n",
    "    # plt.plot(x, probabilities)\n",
    "    # plt.show()    \n",
    "    # curve_area = trapz(probabilities, dx=.01)\n",
    "    # print(\"area =\", curve_area)\n",
    "    tot = sum(probabilities)\n",
    "    probabilities = [x/tot for x in probabilities]\n",
    "\n",
    "    return list(zip(x,probabilities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciao_my_xspec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
